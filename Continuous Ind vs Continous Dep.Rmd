---
title: "R Stats Project"
author: "Alex Chang"
date: "April 8, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

####This is an R Markdown file created to store some of the basic stat techniques I may need to rely on in the future 

##Continous Predictor & Outcome

The most common usage when both your disease predictor and outcome are continuous are correlations.  Typically these are fit linearly, and if not a best fit line can be established to understand what numerical transformation is needed in order to better conduct a linear regression.  

We will need several things, namely a workable data set and some packages.
Let's start with a dataset that's built into R that a lot of my intro stats class uses: Cars.  

```{r}
data <- mtcars 
head(data,10)
```

Let's also utilize some data visualization packages to make sure that we spruce up our review of the analysis later on.  Two common ones are ggplot2 and ggpubr.  

Typically you can request these packages from CRAN, but just to be a little fancy we'll pull directly from their respective github pages.  

```{r eval=FALSE, include=FALSE}
library(devtools)
devtools::install_github("kassambara/ggpubr")
devtools::install_github("tidyverse/ggplot2")
install.packages("ggpubr")
```
The latter code was unnessesary because apparently ggplot2 is a part of the ggpubr git, but better safe than sorry! 
I also ended up just using CRAN because it never gives me an error message if I load a package.   

###Performing Diagnostics for correlations tests 

When performing a parametric test such as correlation, you'd want to establish normality in the data set.  While a perfectly normal set of data is unrealistic, it at least gives you an idea of how robust your correlation may be beyond just the the R or R - squared value.  You effectively have something to reference to justify performing a transformation or using other analysis.  

Let's just start with a scatter plot for now.  Let's compare a car's horsepower with their miles per gallon.

```{r}
library("ggpubr")
ggscatter(data, x= "mpg", y = "hp", 
          color = "red", fill = "lightgray",
          add = "reg.line", conf.int = TRUE,
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Miles per Gallon", ylab = "Horsepower")
```

Look at that, red beauty.  No surprise here, lower horsepower nets a higher mpg rating.  

Now let's check for normality. 

```{r}
shapiro.test(data$mpg)
shapiro.test(data$hp)
```

Looks like horsepower's p-value is <0.05 meaning it is not normal. Not surprising considering out scatter plot would have a trend line that looks like an inverse log function.  But this hardly removes the validity from our analysis given how strong the correlation is.  

How do the Q-Q plots look? 
```{r}
ggqqplot(data$mpg, ylab = "mpg")
ggqqplot(data$hp, ylab = "horsepower")
```

At least the QQ plot for horsepower fits within a normal distribution characterized by the shade of grey.  

This means we can move forward with a person correlation test, should these prove to the be grossly non parametic, a Spearman or Kendall rank-based test would be used.  

###Correlation Tests

```{r}
#Pearson Correlation test 
pearson <- cor.test(data$hp, data$mpg, 
                     method = "pearson")
kendall <- cor.test(data$hp, data$mpg, 
                     method = "kendall")
spearman <- cor.test(data$hp, data$mpg, 
                     method = "spearman")
pearson
kendall
spearman
```

###Linear Regression 

####Simple Linear Regression

Ahhh, the quintessential linear regression.  The workhorse of any basic statistical analysis, and pattern recognition at it's finest.  

Let's load a data set and some packages. 

```{r}
install.packages("tidyverse", repos="http://cran.rstudio.com/", dependencies=TRUE)
library(tidyverse)
library(ggpubr)
theme_set(theme_pubr())
devtools::install_github("kassambara/datarium")
```
```{r}
#loading the data 
data ("marketing", package = "datarium")
head (marketing, 5)
```

```{r}
#Visualize the scatter plot
ggplot(marketing, aes(x = facebook, y = sales)) + 
  geom_point() + 
  stat_smooth()
```

Look linear to me.  Let's just go through the regression

```{r}
reg <- lm(sales ~ facebook, data = marketing)
summary(reg)
confint(reg)
```

Let's interpret our residual standard error, we can express this as a percentage.  \
```{r}
sigma(reg)*100/mean(marketing$sales)
```

about 30% 
This isn't really that great considering that our R-squared is only 0.32.87, meaning that roughly 32% of the change in sales was due to facebook.  As a predictor of sales this isn't good, but in reality this would be great as it would mean a significant proportion of your sales is affected by facebook.  Oh by the way, this is an advertising budget.  

####Multiple Linear Regression 

Now let's take the same dataset and see what a firm's advertising budget should be spent on according to this data set.  

```{r}
multiple <- lm(sales ~ youtube + facebook + newspaper, data = marketing)
summary (multiple)
```


According to our summary output, not facebook and youtube seem to have a positive influence on the predicted sales, with facebook giving it the best boost.  Newspapers seem to not only not produce any positive impact, but negatively impact sales numbers.  


####Model Selection

We can't really talk about multiple linear regression without including one of the key steps: Model selection.  

When it comes to predicting behavior and extrapolating future patterns based on previous observations, refining a regression model is one of the more straight forward methods of doing so with low computing power.  
```{r}
library (MASS)
library (tidyverse)
fullmodel <- lm(Fertility ~ ., data = swiss)
step.model <- stepAIC(fullmodel, direction = "both", trace = FALSE)
head(swiss, 6)
summary(step.model)
```

This stepwise regression looks at the AIC or akaike information criteria, essentially it is an estimator for the relative quality of models.   The lower the value, the better.   Using this method we can see that all the variables in the data set were included in the model as they all had a low AIC cut off determined.  

Let's use the olsrr package to better visualize the selection process.

```{r}
install.packages("olsrr", repos="http://cran.rstudio.com/", dependencies=TRUE)
library(olsrr)
visual <- ols_step_both_p(fullmodel)
plot(visual)
```

As we can see the model is strongest when all four variables are present, meaning that this combination of preditcors in our model produces the most accurate output given out data set.  Of course this doesn't test for one of the common pitfall of multiple regression: Collinearity.   This occurs when two variables have a linear relationship with one another, meaning that estimates could have high error.  The Variance Inflation Factor (VIF)  is a good estimate of that, you typically want it lower than 10 to pass the eye test, and between 1 and 4 to feel confident in a varaible.  Fortunately this package also includes that test.  

```{r}
ols_vif_tol(fullmodel)
```


We're in luck! Our dataset suggestions little to no collinearity!  



